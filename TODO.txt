TODO:
==============

- Exclude some transitive dependency and/or force more recent versions.
  E.g., exclude commons-logging (do it up to Maven Parent).

- Make sure Collector + Importer Temp directories are derived from 
  collector instead of system /tmp when not set. E.g.,
  /workDir/collectorDir/crawlerDir/temp

- Come up with sample prometheus config files and documentation how to use

- Allow runtime scaling via JMX.

- Use Apache HTTPClient 5.x for the default fetcher.

- Sitemap ignore flag should be move intuitive (ignore auto-detection)
  maybe separate detection from sitemap parsing.
  
- Once document reference can be updated anywhere in the process, keeping
  a collection of them (from original to final), then remove support
  for defining a field as source ID in committers.

- Have a flag that says if coming from sitemap OR better: add a field that 
  stores the sitemap file where a page was found.

- Standarsize collector/importer generated metadata field names
  (e.g., "camel-case vs hypens", "collector." vs "document." prefixes).

- Create a class holding constants for frequent XML config attributes. 

- Enforce naming convention where possible (e.g. disabled vs ignored)
  and maybe remove disabled/ignored in favor of null / <selfClosed/>.

- GenericHttpFetcher: rename "headers" to "requestHeaders", rename 
  "headersPrefix" to "responseHeadersPrefix" and add:
  responseHeadersDiscard.  No need to add responseHeadersOnSet since
  http headers are the first thing obtained (or do it since we could get
  both HEAD and GET headers).

- Document that the data store can be used for anything now, not just crawl data.

- CrawlReference is casted to HttpCrawlReference in many places.
  Use generic in Collector Core to prevent casting?

- <referenceFilters> should be urlFilters to avoid conflict with
  Importer equivalent?

- Eliminate CachedInputStream.dispose() in favor of close() to be consistent?

- Add a reference filter that matches URLs with the same segment repeated
  Nth consecutive times.

- Do lib dups check on startup.

- Fix javadoc (including links) all over.

- Add xdoclet or equivalent that could be extracted from java doc to build
  online doc automatically/dynamically.

- Add ability to disable storing HTTP headers as metadata fields.

- Add update.sh scripts to collectors (doing the same as install.sh script)

- Provide generic way to access JSON APIs (using standard auth methods, 
  ability to provide how to access next pages, etc).

- Provide support for new popular auth methods.

- Offer to overwrite httpFetcher-specified User-Agent directly on 
  classes that needs it, like the Robots.txt related class.

- Try version 2.12.0 of apache xerces that came up in June 2018.

- Where appropriate, remove the many places where we pass a collector or 
  http config in favor of those classes being able to get it on startup or 
  statically.

- Remove stop() methods in a few classes and listen for crawler/collector
  shutdown instead. 

- MAYBE: Upon encountering specific warnings, make command line interactive
  by default (unless headless detected) and provide option to turn off.

- Optionally keep the URL trail or at least start URL for document:
  https://github.com/Norconex/collector-http/issues/514

- Have ability to download and install committers from command line?

- Have ability to do POST instead of GET for matching URLs.

- Because the QueuePipeline can be invoked from anywhere, make the 
  "inscope" check part of it instead of always forcing to check first.

- Support HTTP v2.  Example site: https://www.eetimes.com/rss_simple.asp

- Add external site depth?

- Add one of these as JVM arg: 
      -XX:ExitOnOutOfMemoryError
      -XX:CrashOnOutOfMemoryError 

- Add to FAQ: how to extract few different HTML tag scenarios and maybe 
  add a tagger that helps with that.

- Maybe rename REJECTED_UNMODIFIED|PREMATURE to IGNORED_* ? 

- Have a new <inScopeStrategy class="..."> with the default implementation
  being [Generic]URLCrawlScopeStrategy, taking the stayOn* and implementing 
  an IURLCrawlScope interface.

- Investigate why defining tags (e.g. <httpClientFactory> under the collector
  root tag (as opposed to under crawler tag) does not produce validation error.

- Image cache gives issues with multiple crawlers using the same path, fix this.

- Make it possible to merge two or more docs.

- Maybe add "stayOnDomainMaxSub" for how many subdomain level to support?
  (default would be zero).

- Fix redirects used to authenticate or create session but otherwise return
  to original URL which then can be crawled.

- Use Apache HttpClient-Win for Windows Auth support like done with
  Azure Search Committer.

- Have the config reference.xml put all settings in a crawler instead
  of crawling defaults (FS Collector as well).

- Ship with importer.[sh|bat] files (applies to FS Collector as well).

- Have a way to store all data for easy recrawl.

- On GenericLinkExtractor, offer a flag to convert \ to /.  Cannot be done
  at normalization time since relative URLs will not be converted to full
  URLs properly without that.

- Consider deprecating TikaLinkExtractor, or at least update it to support
  relative base hrefs (share some code with GenericLinkExtractor).

- Offer a separate process to run on the crawl store and produce some kind of 
  flat file with referring URL counts so people can decide to use this
  to create some form of relevancy in their search engine (e.g. using 
  inbound links as a popularity indicator).

- Have a global settings for making Properties (HttpMetadata, 
  ImporterMetadata, etc) case sensitive or not.

- To consider: when re-processing orphans, check the depth in case the 
  max depth changed.

- Offer a flag to merge frame/iframe elements on HTML pages as opposed to
  crawl them as separate documents.

- Carry a flag that tells if we are running a full crawl, or an incremental one.
  Some operations may be relevant only on incremental runs.

- Store somewhere in metadata (and maybe carry in code?) whether a 
  document is new or modified.

- When issuing a STOP, add an option to specify which crawler to stop,
  and let others run.

- Add a startup flag that will generate a batch/sh script for the user which 
  abstracts the call to the config file 
  (e.g., abcCollector.sh start|stop|resume)

- Ability to crawl up to a given size.  Absolute number or percentage of 
  disk capacity? Shall we tie that to checking for remaining space? 
  We could issue warnings and/or stop when threshold is reached to 
  prevent crashing due to lack of space.

- Introduce "Add-ons" like social media add-ons to crawl social media sites.

- Have an interface for how to optionally store downloaded files
  (i.e., location, directory structure, file naming).  This could allow
  usage of the collector to clone a site.  Should the DocumentFetcher do it 
  instead?

- Have a crawler event listener that generated a tree-like graph of all URLs
  (i.e. a kind of sitemap).
  
- To consider: Interface for how to save documents when they are kept.
  Same with default committer queue location.
  File system is used for both now, but could be others like MongoDB?
  Same as previous item?   

- Add support for having different HTTPContext for each call and/or each
  sites.
  
- Add the ability to control how many successive crawls it has to go through
  before deleting a document because it was not found (404).

- Have configurable the level of verbosity desired when logging exceptions.
  The options could be:
     - type: none|all|first|last
     - stacktrace: false|true

- If a URL filter rule was changed and a document is now rejected (never 
  processed), it will not be deleted (since it did not get a 404/NOT_FOUND).
  Maybe check if rejected URL in URLProcessor are in cache and send deletion 
  if so.

- Integrate with distributed computing/storing/streaming frameworks such as 
  Spark/Hadoop/Kafka.

- Test that IPV6 is supported (as domain names).

- Offer option to have crawling rate adjust itself if a site is slow
  (in case it is the crawler hammering it).  Probably a change or new delay
  implementation... this means total download time (both for HEAD and GET) 
  should be added as document properties (not a bad idea to do regardless).

- Add option to skip certain http response code (like 500).  Those docs 
  that should not be added nor deleted because they are in a temporary bad 
  state.  Is it really useful?

- Deal with <a rel="noreferrer" ... ??

MAYBE:
=======
- Start offering per-site options?  Like crawl interval and more?
  (can be achieve with defining different crawlers now).
